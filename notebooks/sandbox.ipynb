{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runing\n"
     ]
    }
   ],
   "source": [
    "print('runing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bellatini/miniconda3/envs/dl3d/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write state dict\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'setup done'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from partglot.utils.predict import get_loaded_model\n",
    "import numpy as np\n",
    "from src.helper.visualization import visualize_pointclouds_parts_partglot\n",
    "\n",
    "import pymeshlab as pm\n",
    "import torch\n",
    "from partglot.utils.neural_utils import tokenizing\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "np.random.seed(0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "part_names = [\"back\", \"seat\", \"leg\", \"arm\"]\n",
    "\n",
    "part_semantic_groups = {\n",
    "    \"back\": [\"back\"],\n",
    "    \"seat\": [\"seat\"],\n",
    "    \"leg\": [\"leg\", \"wheel\", \"base\"],\n",
    "    \"arm\": [\"arm\"],\n",
    "}\n",
    "\n",
    "model_dir = \"/home/bellatini/DL3D-Practical/models/pn_agnostic.ckpt\"\n",
    "data_dir = \"/home/bellatini/DL3D-Practical/data/partglot\"\n",
    "# data_dir = \"/home/bellatini/DL3D-Practical/Baselines/PartGlot/data\"\n",
    "\n",
    "partglot, partglot_dm = get_loaded_model(data_dir=data_dir, model_path=model_dir)\n",
    "sup_segs2label, pc2label = partglot.get_attn_maps()[1]\n",
    "segs, masks = partglot_dm.h5_data['data'][1], partglot_dm.h5_data['mask'][1]\n",
    "\n",
    "partglot.to(device)\n",
    "\n",
    "\"setup done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_arrays(arrays):\n",
    "    ref_array = arrays[0]\n",
    "    sorted_indices = ref_array.argsort()\n",
    "    out = []\n",
    "    for a in arrays:\n",
    "        out.append(a[sorted_indices])\n",
    "    return out\n",
    "\n",
    "def random_sample_array(arr: np.array, size: int = 1, with_replacement:bool=True) -> np.array:\n",
    "    if with_replacement:\n",
    "        while len(arr) < size:\n",
    "            arr = np.concatenate([arr, arr])\n",
    "    return arr[np.random.choice(len(arr), size=size, replace=False)]\n",
    "\n",
    "def get_attn_mask_objects(partglot_pointcloud, pc2label):\n",
    "    \"\"\"\n",
    "    Returns ordered point cloud and mask indices in our format.\n",
    "    \"\"\"\n",
    "    stacked_pc = np.vstack(partglot_pointcloud)\n",
    "    \n",
    "    arg_sort = pc2label.argsort()\n",
    "    \n",
    "    out_pc2label, out_pg_pc = pc2label[arg_sort], np.vstack(stacked_pc)[arg_sort]\n",
    "\n",
    "    mask = {}\n",
    "    for i, pn in enumerate(part_names):\n",
    "        tmp = np.where(out_pc2label == i)[0]\n",
    "        mask[pn] = [tmp.min(), tmp.max()]\n",
    "    \n",
    "    return {\"mask_vertices\": mask}, out_pg_pc\n",
    "\n",
    "def cluster_supsegs(sorted_labels, sorted_pc, sup_seg_size=512):\n",
    "    sup_segs, labels = [], []\n",
    "    for lbl in np.unique(sorted_labels):\n",
    "        indices = np.where(sorted_labels==lbl)[0]\n",
    "        tmp_pc = sorted_pc[indices]\n",
    "        tmp_lbl = sorted_labels[indices]\n",
    "        sup_segs.append(random_sample_array(tmp_pc, sup_seg_size))\n",
    "        labels.append(random_sample_array(tmp_lbl, sup_seg_size))\n",
    "    return np.array(sup_segs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(partglot_dm\u001b[39m.\u001b[39mh5_data[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m:\u001b[39m3\u001b[39m])\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m mask_data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(partglot_dm\u001b[39m.\u001b[39mh5_data[\u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m2\u001b[39m:\u001b[39m3\u001b[39m])\u001b[39m.\u001b[39munsqueeze(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> 4\u001b[0m pc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((np\u001b[39m.\u001b[39mvstack(np\u001b[39m.\u001b[39;49mvstack(batch_data\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39;49mnumpy()))))\n\u001b[1;32m      6\u001b[0m kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mfit(pc)\n\u001b[1;32m      7\u001b[0m pc2sup_segs \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39mlabels_\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl3d/lib/python3.9/site-packages/numpy/core/shape_base.py:282\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(arrs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    281\u001b[0m     arrs \u001b[39m=\u001b[39m [arrs]\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "batch_data = torch.from_numpy(partglot_dm.h5_data['data'][1:3]).unsqueeze(dim=1).float().to(device)\n",
    "mask_data = torch.from_numpy(partglot_dm.h5_data['mask'][1:3]).unsqueeze(dim=1).float().to(device)\n",
    "\n",
    "pc = np.vstack((np.vstack(np.vstack(batch_data.cpu().numpy()))))\n",
    "\n",
    "kmeans = KMeans(n_clusters=25, random_state=1).fit(pc)\n",
    "pc2sup_segs = kmeans.labels_\n",
    "np.unique(pc2sup_segs, return_counts=True)\n",
    "\n",
    "sorted_labels, sorted_pc = sort_arrays((kmeans.labels_, pc))\n",
    "\n",
    "super_segs, _ = cluster_supsegs(sorted_labels, sorted_pc)\n",
    "\n",
    "custom_ssegs_batch = torch.from_numpy(np.array([[super_segs]])).float().to(device)\n",
    "custom_mask_batch = torch.from_numpy(np.array([[np.ones(custom_ssegs_batch.shape[2])]])).float().to(device)\n",
    "custom_ssegs_batch.shape, custom_mask_batch.shape\n",
    "\n",
    "attn_maps = []\n",
    "for pn in part_names:\n",
    "    text_embeddings = tokenizing(partglot_dm.word2int, f\"chair with a {pn}\").to(device)[None].expand(\n",
    "        1, -1\n",
    "    )\n",
    "    tmp = partglot.forward(\n",
    "        batch_data, # custom_ssegs_batch / batch_data\n",
    "        mask_data, # custom_mask_batch / mask_data\n",
    "        text_embeddings, True)\n",
    "    attn_maps.append(tmp)\n",
    "    \n",
    "sup_segs2label = np.squeeze(torch.cat(attn_maps).max(0)[1].cpu().numpy())\n",
    "sup_segs2label\n",
    "\n",
    "# custom_ssegs_batch\n",
    "super_segs.shape\n",
    "# custom_ssegs_batch.shape\n",
    "K, n_points, coord = super_segs.shape\n",
    "\n",
    "pc2sup_segs=[]\n",
    "for ki in range(K):\n",
    "    tmp = np.ones(n_points) * ki\n",
    "    pc2sup_segs.append(tmp)\n",
    "    \n",
    "pc2sup_segs = np.concatenate(pc2sup_segs).astype(int)\n",
    "\n",
    "assign_ft = lambda x: sup_segs2label[x]\n",
    "\n",
    "pc2label = assign_ft(pc2sup_segs)\n",
    "\n",
    "final_mask, final_pc = get_attn_mask_objects(super_segs, pc2label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for s,f in final_mask['mask_vertices'].values():\n",
    "    tmp = final_pc[s:f]\n",
    "    out.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pc2label_ref = np.load(\"/home/bellatini/DL3D-Practical/Baselines/PartGlot/logs/pre_trained/pn_agnostic/12-12_14-37-13/pred_label/final/0_pc_label.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 1, 50])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cat(attn_maps).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8783cb6fb8d5493b98ec5e407b5783e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_pointclouds_parts_partglot(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21654a8b7d324c3c95b4df41bf41eb9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_pointclouds_parts_partglot(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc2label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dl3d')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5edac632bc030e698bde792f49d6429b3d15e7fbac06b4787d1057cf5bdd41d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
